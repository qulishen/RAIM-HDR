<p align="center">
  <img src="logo.png" alt="RAIM-HDR Logo" width="200"/>
</p>

<h1 align="center">ğŸŒŸ RAIM-HDR: Multi-Exposure Image Fusion in Dynamic Scenes </h1>

<p align="center">
  <a href="README.md"><img src="https://img.shields.io/badge/Language-English-blue" alt="English"></a>
  <a href="README_cn.md"><img src="https://img.shields.io/badge/è¯­è¨€-ä¸­æ–‡-red" alt="ä¸­æ–‡"></a>
  <a href="https://www.codabench.org/competitions/12728/"><img src="https://img.shields.io/badge/ğŸ†_Competition-CodaBench-orange" alt="Competition"></a>
</p>

---

This repository provides the baseline code for **The 3rd Restore Any Image Model (RAIM) Challenge: Multi-Exposure Image Fusion in Dynamic Scenes (Track 2)** ğŸ¯

ğŸ”— **Competition Link**: [https://www.codabench.org/competitions/12728/](https://www.codabench.org/competitions/12728/)

## ğŸ“– Overview

The goal of this track is to fuse multiple LDR (Low Dynamic Range) images captured at different exposure levels into a single HDR (High Dynamic Range) image, handling dynamic scenes with potential motion artifacts. âœ¨

Our baseline model is based on the **Uformer** architecture, adapted for multi-exposure image fusion tasks. ğŸš€

## ğŸ“¦ Dataset

### ğŸ“¥ Download

Download the dataset from: [www.example.com](https://www.example.com)

### ğŸ—‚ï¸ Structure

The dataset should be organized as follows:

```
datasets/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ 001/
â”‚   â”‚   â”œâ”€â”€ 0.jpg      # ğŸŒ‘ Darkest exposure
â”‚   â”‚   â”œâ”€â”€ 1.jpg
â”‚   â”‚   â”œâ”€â”€ 2.jpg
â”‚   â”‚   â”œâ”€â”€ 3.jpg      # âš–ï¸ Middle exposure
â”‚   â”‚   â”œâ”€â”€ 4.jpg
â”‚   â”‚   â”œâ”€â”€ 5.jpg
â”‚   â”‚   â””â”€â”€ 6.jpg      # â˜€ï¸ Brightest exposure
â”‚   â”‚   â””â”€â”€ HDR.jpg    # ğŸ¨ HDR image
â”‚   â”œâ”€â”€ 002/
â”‚   â”‚   â”œâ”€â”€ 0.jpg
â”‚   â”‚   â”œâ”€â”€ ...
â”‚   â”‚   â””â”€â”€ 6.jpg
â”‚   â”‚   â””â”€â”€ HDR.jpg    # ğŸ¨ HDR image
â”‚   â””â”€â”€ ...
â””â”€â”€ test/
    â”œâ”€â”€ 001/
    â”‚   â”œâ”€â”€ 1.jpg
    â”‚   â”œâ”€â”€ ...
    â”‚   â””â”€â”€ 5.jpg
    â””â”€â”€ ...
```

Each scene folder of training dataset contains 7 images (0.jpg to 6.jpg) captured at different exposure levels, ordered from darkest to brightest. ğŸ“¸

## ğŸ› ï¸ Installation

### ğŸ“‹ Requirements

- ğŸ Python >= 3.8
- ğŸ”¥ PyTorch >= 1.12
- âš¡ CUDA >= 11.3

### ğŸ“¦ Install Dependencies

```bash
pip install -r requirements.txt
```

Additional dependencies for training:

```bash
pip install opencv-python-headless timm scipy einops accelerate lmdb ftfy tqdm Pillow tensorboard
```

## ğŸš€ Usage

### ğŸ‹ï¸ Training

Start training with the provided script:

```bash
./train.sh
```

Or run manually with custom settings:

```bash
CUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node=2 --master_port=4392 \
    basicsr/train.py -opt options/Uformer.yml --launcher pytorch --auto_resume
```

**Key training parameters** (configured in `options/Uformer.yml`):

| Parameter            | Value  | Description               |
| -------------------- | ------ | ------------------------- |
| `batch_size_per_gpu` | 8      | Batch size per GPU        |
| `gt_size`            | 256    | Training patch size       |
| `total_iter`         | 300000 | Total training iterations |
| `lr`                 | 2e-4   | Learning rate             |
| `ema_decay`          | 0.999  | EMA decay rate            |

### ğŸ§ª Testing / Inference

Run inference on test images:

```bash
python test_crop.py \
    --input_dir /path/to/test/images \
    --output_dir /path/to/output \
    --weights /path/to/checkpoint.pth \
    --opt options/Uformer.yml \
    --crop_size 256 \
    --crop_shave 32 \
    --factor 16
```

**Arguments:**

| Argument       | Default | Description                                |
| -------------- | ------- | ------------------------------------------ |
| `--input_dir`  | -       | Root folder containing sequence subfolders |
| `--output_dir` | -       | Output directory for fused HDR images      |
| `--weights`    | -       | Path to trained model checkpoint           |
| `--opt`        | -       | YAML config file path                      |
| `--crop_size`  | 256     | Block size for tiled inference             |
| `--crop_shave` | 32      | Overlap size for blending tiles            |
| `--factor`     | 16      | Padding factor for network stride          |

### ğŸ“Š Evaluation

Evaluate predictions against ground truth:

```bash
python eval.py \
    --pred_root /path/to/predictions \
    --gt_root /path/to/ground_truth \
    --device auto
```

**Arguments:**

| Argument      | Default | Description                              |
| ------------- | ------- | ---------------------------------------- |
| `--pred_root` | -       | Directory containing predicted images    |
| `--gt_root`   | -       | Directory containing ground truth images |
| `--device`    | auto    | Device selection (auto/cpu/cuda)         |

**Evaluation Metrics:** ğŸ“ˆ

- ğŸ“ **PSNR**: Peak Signal-to-Noise Ratio
- ğŸ” **SSIM**: Structural Similarity Index
- ğŸ§  **LPIPS**: Learned Perceptual Image Patch Similarity
- ğŸ† **Score**: Weighted combination following NTIRE formula

## ğŸ¬ Example & Challenge Focus

<p align="center">
  <img src="example.png" alt="HDR Fusion Example" width="100%"/>
</p>

The figure above illustrates the key challenges in multi-exposure HDR fusion. Participants can improve their models from **two main aspects**:

### ğŸ”´ Dynamic Range Recovery (Red Box)

| Challenge | Description |
|-----------|-------------|
| ğŸŒ‘ **Under-exposure** | Dark regions lose details and appear noisy |
| â˜€ï¸ **Over-exposure** | Bright regions are saturated and washed out |
| ğŸ¯ **Goal** | Recover full dynamic range with rich details in both shadows and highlights |

### ğŸŸ¡ Motion Ghosting Removal (Yellow Box)

| Challenge | Description |
|-----------|-------------|
| ğŸ‘» **Ghosting artifacts** | Moving objects appear as semi-transparent duplicates |
| ğŸƒ **Motion blur** | Fast-moving subjects cause blurry edges |
| ğŸ¯ **Goal** | Produce ghost-free HDR images with sharp moving objects |

## ğŸ“ Project Structure

```
RAIM-HDR/
â”œâ”€â”€ basicsr/                 # ğŸ”§ Core library
â”‚   â”œâ”€â”€ archs/              # ğŸ—ï¸ Network architectures (Uformer, etc.)
â”‚   â”œâ”€â”€ data/               # ğŸ“Š Dataset loaders
â”‚   â”œâ”€â”€ losses/             # ğŸ“‰ Loss functions
â”‚   â”œâ”€â”€ metrics/            # ğŸ“ Evaluation metrics
â”‚   â”œâ”€â”€ models/             # ğŸ¤– Training models
â”‚   â”œâ”€â”€ ops/                # âš™ï¸ Custom CUDA operators
â”‚   â”œâ”€â”€ utils/              # ğŸ› ï¸ Utility functions
â”‚   â”œâ”€â”€ train.py            # ğŸ‹ï¸ Training script
â”‚   â””â”€â”€ test.py             # ğŸ§ª Testing script
â”œâ”€â”€ datasets/               # ğŸ“¦ Dataset folder
â”œâ”€â”€ options/                # âš™ï¸ Configuration files
â”‚   â””â”€â”€ Uformer.yml        # ğŸ“ Uformer training config
â”œâ”€â”€ train.sh               # ğŸš€ Training launch script
â”œâ”€â”€ test_crop.py           # ğŸ”¬ Inference script with tiling
â”œâ”€â”€ eval.py                # ğŸ“Š Evaluation script
â”œâ”€â”€ requirements.txt       # ğŸ“‹ Python dependencies
â””â”€â”€ README.md
```

## ğŸ“„ License

This project is released for academic research purposes. ğŸ“

## ğŸ™ Acknowledgments

- [BasicSR](https://github.com/XPixelGroup/BasicSR) - The training framework ğŸ’ª
- [Uformer](https://github.com/ZhendongWang6/Uformer) - The backbone architecture ğŸŒŸ

---

<p align="center">
  <b>Good luck with the challenge! ğŸ€ğŸ‰</b>
</p>
